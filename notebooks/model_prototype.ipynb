{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d269f49f-edda-4d60-9bb3-800e6f7d737a",
   "metadata": {},
   "source": [
    "# üß† Real-time Entity Classifier - CNN Architecture\n",
    "\n",
    "This notebook defines the convolutional neural network (CNN) architecture that will classify webcam frames into one of the following categories:\n",
    "\n",
    "- üê± **Pet** (my girlfriend's sphynx cat, Lucy)\n",
    "- üë§ **Pet Owner** (me, Sebastian)\n",
    "- üßç **Another Person**  \n",
    "- üö´ **Nobody Present**\n",
    "\n",
    "The CNN architecture is designed to extract meaningful features from real-time video input and make an accurate classification based on the detected subject.  \n",
    "It is lightweight and suitable for live webcam processing while respecting user privacy. üîí"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7efa1e-0ec7-4f5a-8669-b8f858f56391",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d4b3a2-c363-4c1e-947b-a83b70150fb7",
   "metadata": {},
   "source": [
    "## üì¶ PyTorch Imports Overview\n",
    "\n",
    "To build and train our convolutional neural network (CNN), we import key components from the **PyTorch** framework:\n",
    "\n",
    "- **`torch`**: The main PyTorch library, which provides core functionalities such as tensor operations, model creation, and automatic differentiation. It is the backbone of PyTorch.\n",
    "\n",
    "- **`torch.nn`**: This module offers essential tools for building neural networks, such as `Conv2d` for convolutional layers, `Linear` for fully connected layers, and many other layers and utilities that help define and train models.\n",
    "\n",
    "- **`torch.nn.functional`**: A functional interface to neural network operations. It includes commonly used functions like `ReLU`, `Sigmoid`, and others, providing flexibility and stateless operations. These functions are typically used directly in the `forward()` method of a model.\n",
    "\n",
    "- **`torch.optim`**: Contains optimization algorithms like Adam, SGD, and more. These are used to update the model‚Äôs weights during training, minimizing the loss function.\n",
    "\n",
    "- **`torchvision.transforms`**: A collection of image transformation utilities that allow us to preprocess and augment image data, like resizing, normalization, and random flips. This is essential when working with image datasets.\n",
    "\n",
    "- **`torchvision.datasets`**: Provides easy access to popular datasets. In this project, we use **`datasets.ImageFolder`**, which is designed to load images organized into class-specific folders. It helps us load and manage custom datasets where images are stored in a directory structure.\n",
    "\n",
    "- **`DataLoader`**: A utility that simplifies data loading, batching, and shuffling, ensuring that our data is efficiently processed during training and evaluation.\n",
    "\n",
    "- **`tqdm`**: A Python library that adds a progress bar to loops, making it easy to monitor the progress of long-running operations, such as training over multiple epochs.\n",
    "\n",
    "- **`matplotlib.pyplot`**: A plotting library used to visualize key metrics like training loss and accuracy over time, helping us understand the model‚Äôs performance during training.\n",
    "\n",
    "These imports form the foundation for constructing, training, and evaluating our deep learning model. üß†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "280d0aba-b847-4435-bdfc-318962f346d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary libraries for deep learning with PyTorch\n",
    "\n",
    "# Importing torch for tensor operations and model creation\n",
    "import torch\n",
    "\n",
    "# Importing neural network modules to build and train neural networks\n",
    "import torch.nn as nn\n",
    "\n",
    "# Importing functional APIs from PyTorch for operations like activation functions\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Importing torch.optim for various optimization algorithms (like Adam, SGD)\n",
    "import torch.optim as optim\n",
    "\n",
    "# Importing transforms and datasets for image processing and loading standard datasets\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "\n",
    "# Importing DataLoader to handle batching and shuffling of data\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Importing tqdm to visualize progress in loops (like during training)\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Importing matplotlib for plotting graphs and visualizing metrics\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fd8046-b414-4872-9661-533e52733b24",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26de83e6-f723-4e42-bd24-bcd550690961",
   "metadata": {},
   "source": [
    "## üß† CNN Model Definition & Architecture Explanation\n",
    "\n",
    "We define a Convolutional Neural Network (CNN) model called `EntityClassifierCNN`, designed to classify webcam frames into one of four categories:\n",
    "\n",
    "### üéØ Classification Targets:\n",
    "1. **Pet** ‚Äì my girlfriend‚Äôs Sphynx cat, Lucy üê±  \n",
    "2. **Owner** ‚Äì me, Sebastian üë®  \n",
    "3. **Other Person** ‚Äì anyone who is not the owner üßç  \n",
    "4. **None** ‚Äì when no person or pet is present üö´\n",
    "\n",
    "### üß± Model Architecture Overview:\n",
    "\n",
    "The model follows a standard convolutional architecture:\n",
    "\n",
    "- **Three Convolutional Layers**:\n",
    "  - Each convolution uses a **3√ó3 kernel** with **padding of 1** to preserve spatial dimensions, and **stride being 1** by default.\n",
    "  - The number of output feature maps increases across layers: **32 ‚Üí 64 ‚Üí 128**.\n",
    "  - After each convolution, a **ReLU activation** is applied, followed by **2√ó2 MaxPooling**, which halves the spatial resolution.\n",
    "\n",
    "- **Fully Connected Layers**:\n",
    "  - The output of the final convolutional layer is **flattened** and passed to a fully connected layer with **512 neurons**.\n",
    "  - A **Dropout layer** with a probability of **0.3** is applied to reduce overfitting.\n",
    "  - The final fully connected layer outputs **logits for 4 classes**, one for each possible classification.\n",
    "\n",
    "### üîÅ Forward Pass Flow:\n",
    "\n",
    "1. Input image (assumed to be RGB, 3 channels)  \n",
    "2. Conv ‚Üí ReLU ‚Üí MaxPool  \n",
    "3. Conv ‚Üí ReLU ‚Üí MaxPool  \n",
    "4. Conv ‚Üí ReLU ‚Üí MaxPool  \n",
    "5. Flatten  \n",
    "6. Fully connected ‚Üí ReLU ‚Üí Dropout  \n",
    "7. Final fully connected layer ‚Üí Output scores for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e6ad088d-dee9-443e-b5e1-f63a098e21c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityClassifierCNN(nn.Module):\n",
    "    def __init__(self, num_classes=4):\n",
    "        super(EntityClassifierCNN, self).__init__()  # Call the constructor of nn.Module\n",
    "        \n",
    "        # MaxPooling layer with kernel_size=2 and stride=2\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Convolutional Layer 1: \n",
    "        # in_channels=3 (RGB), out_channels=32, kernel_size=3x3, padding=1\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Convolutional Layer 2: \n",
    "        # in_channels=32 (from previous layer), out_channels=64, kernel_size=3x3, padding=1\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Convolutional Layer 3:\n",
    "        # in_channels=64, out_channels=128, kernel_size=3x3, padding=1\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Fully Connected Layer 1:\n",
    "        # input features = 128 channels * 28 * 28 (flattened), output features = 512\n",
    "        self.fc1 = nn.Linear(in_features=128 * 28 * 28, out_features=512)\n",
    "        \n",
    "        # Fully Connected Layer 2 (Output Layer):\n",
    "        # input features = 512, output features = num_classes (default 4)\n",
    "        self.fc2 = nn.Linear(in_features=512, out_features=num_classes)\n",
    "        \n",
    "        # Dropout with probability = 0.3\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # Output: 112x112\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # Output: 56x56\n",
    "        x = self.pool(F.relu(self.conv3(x)))  # Output: 28x28\n",
    "        x = x.view(-1, 128 * 28 * 28)         # Flatten\n",
    "        x = self.dropout(F.relu(self.fc1(x))) # Apply dropout after activation\n",
    "        return self.fc2(x)                    # Output logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d361f0a-a50a-4360-9844-50151d65ad6c",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7029fb06-3a55-45ab-91a3-02b06e8a6a00",
   "metadata": {},
   "source": [
    "## Model, Optimizer, and Loss Function Setup\n",
    "\n",
    "### 1. **Model Instantiation** üßë‚Äçüíª\n",
    "We instantiate the `EntityClassifierCNN` model\n",
    "\n",
    "### 2. **Optimizer Setup** ‚öôÔ∏è\n",
    "We set up our optimizer for this task as **Adam** üöÄ, a popular and highly efficient choice for training deep learning models. The Adam optimizer adapts the learning rate for each parameter during training, making it more efficient and faster in practice compared to other optimizers like **SGD** (Stochastic Gradient Descent). Unlike SGD, which uses a fixed learning rate for all parameters, Adam dynamically adjusts the learning rate based on individual parameter updates, which helps achieve faster convergence and better performance on complex tasks. We specify a learning rate of **0.001** as a starting point, but this can be fine-tuned based on experimentation.\n",
    "\n",
    "### 3. **Loss Function Setup** ‚öñÔ∏è\n",
    "For our multi-class classification task, we use **CrossEntropyLoss** as our loss function. Cross-entropy loss combines `log_softmax` and `NLLLoss` to efficiently calculate the difference between the predicted class probabilities and the true labels. It is well-suited for multi-class classification problems because it directly optimizes the model to correctly classify each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ba882d62-0c1f-44c9-ac41-7752c5b79b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We instantiate our model\n",
    "model = EntityClassifierCNN()\n",
    "\n",
    "# We use the Adam optimizer since it adapts the learning rate for each parameter and is usually more efficient in practice\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# We will also use the CrossEntropyLoss criterion/loss function due to us having a multi-class classification task\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4363382a-8f8a-4992-91ad-1ad931ad1f6b",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8ed4c8-db25-479a-80a9-eb33265be1e9",
   "metadata": {},
   "source": [
    "## üì¶ DataLoader and Dataset Setup\n",
    "\n",
    "Before training our model, we need to properly load and preprocess the image data. This is handled using PyTorch‚Äôs `ImageFolder` and `DataLoader` utilities.\n",
    "\n",
    "### üñºÔ∏è Image Preprocessing\n",
    "We define a series of transformations to ensure consistency and improve model performance:\n",
    "- **Resize**: All images are resized to a fixed dimension of **224x224** pixels. This is a common size for CNNs and ensures a uniform input shape.\n",
    "- **ToTensor**: Images are converted into PyTorch tensors, enabling them to be processed by the model.\n",
    "- **Normalize**: Pixel values are normalized using the mean and standard deviation from the ImageNet dataset:\n",
    "  - Mean: `[0.485, 0.456, 0.406]`\n",
    "  - Standard Deviation: `[0.229, 0.224, 0.225]`\n",
    "  This helps with model convergence and consistency.\n",
    "\n",
    "### üóÇÔ∏è Dataset Structure\n",
    "The dataset is organized using folders representing each class. Each subdirectory (e.g., `nobody`, `pet`, `owner`, `other_person`) contains images specific to that class. PyTorch‚Äôs `ImageFolder` automatically maps these folders to class labels.\n",
    "\n",
    "### üõ†Ô∏è DataLoader Creation\n",
    "We create two `DataLoader` objects:\n",
    "- **Training Loader**: Loads the dataset in shuffled batches of 32 images to ensure randomness and improve generalization.\n",
    "- **Validation Loader**: Loads the validation data without shuffling, maintaining the order for consistent evaluation.\n",
    "\n",
    "The `num_workers=4` setting allows data loading to happen in parallel for better performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c7888ee1-ff96-4f93-b548-823b7720630e",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Found no valid file for the classes nobody, other_person, owner, pet. Supported extensions are: .jpg, .jpeg, .png, .ppm, .bmp, .pgm, .tif, .tiff, .webp",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[71], line 9\u001b[0m\n\u001b[0;32m      2\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m      3\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m)),  \u001b[38;5;66;03m# Resize images to 224x224\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),  \u001b[38;5;66;03m# Convert images to PyTorch tensors\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize(mean\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.485\u001b[39m, \u001b[38;5;241m0.456\u001b[39m, \u001b[38;5;241m0.406\u001b[39m], std\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m])  \u001b[38;5;66;03m# Normalize\u001b[39;00m\n\u001b[0;32m      6\u001b[0m ])\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Load the dataset\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mImageFolder(root\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data\u001b[39m\u001b[38;5;124m'\u001b[39m, transform\u001b[38;5;241m=\u001b[39mtransform)\n\u001b[0;32m     10\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mImageFolder(root\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data\u001b[39m\u001b[38;5;124m'\u001b[39m, transform\u001b[38;5;241m=\u001b[39mtransform)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Create DataLoader for training\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torchvision\\datasets\\folder.py:328\u001b[0m, in \u001b[0;36mImageFolder.__init__\u001b[1;34m(self, root, transform, target_transform, loader, is_valid_file, allow_empty)\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    321\u001b[0m     root: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    326\u001b[0m     allow_empty: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    327\u001b[0m ):\n\u001b[1;32m--> 328\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    329\u001b[0m         root,\n\u001b[0;32m    330\u001b[0m         loader,\n\u001b[0;32m    331\u001b[0m         IMG_EXTENSIONS \u001b[38;5;28;01mif\u001b[39;00m is_valid_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    332\u001b[0m         transform\u001b[38;5;241m=\u001b[39mtransform,\n\u001b[0;32m    333\u001b[0m         target_transform\u001b[38;5;241m=\u001b[39mtarget_transform,\n\u001b[0;32m    334\u001b[0m         is_valid_file\u001b[38;5;241m=\u001b[39mis_valid_file,\n\u001b[0;32m    335\u001b[0m         allow_empty\u001b[38;5;241m=\u001b[39mallow_empty,\n\u001b[0;32m    336\u001b[0m     )\n\u001b[0;32m    337\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torchvision\\datasets\\folder.py:150\u001b[0m, in \u001b[0;36mDatasetFolder.__init__\u001b[1;34m(self, root, loader, extensions, transform, target_transform, is_valid_file, allow_empty)\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(root, transform\u001b[38;5;241m=\u001b[39mtransform, target_transform\u001b[38;5;241m=\u001b[39mtarget_transform)\n\u001b[0;32m    149\u001b[0m classes, class_to_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfind_classes(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot)\n\u001b[1;32m--> 150\u001b[0m samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_dataset(\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot,\n\u001b[0;32m    152\u001b[0m     class_to_idx\u001b[38;5;241m=\u001b[39mclass_to_idx,\n\u001b[0;32m    153\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mextensions,\n\u001b[0;32m    154\u001b[0m     is_valid_file\u001b[38;5;241m=\u001b[39mis_valid_file,\n\u001b[0;32m    155\u001b[0m     allow_empty\u001b[38;5;241m=\u001b[39mallow_empty,\n\u001b[0;32m    156\u001b[0m )\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader \u001b[38;5;241m=\u001b[39m loader\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextensions \u001b[38;5;241m=\u001b[39m extensions\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torchvision\\datasets\\folder.py:203\u001b[0m, in \u001b[0;36mDatasetFolder.make_dataset\u001b[1;34m(directory, class_to_idx, extensions, is_valid_file, allow_empty)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m class_to_idx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;66;03m# prevent potential bug since make_dataset() would use the class_to_idx logic of the\u001b[39;00m\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;66;03m# find_classes() function, instead of using that of the find_classes() method, which\u001b[39;00m\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;66;03m# is potentially overridden and thus could have a different logic.\u001b[39;00m\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe class_to_idx parameter cannot be None.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 203\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m make_dataset(\n\u001b[0;32m    204\u001b[0m     directory, class_to_idx, extensions\u001b[38;5;241m=\u001b[39mextensions, is_valid_file\u001b[38;5;241m=\u001b[39mis_valid_file, allow_empty\u001b[38;5;241m=\u001b[39mallow_empty\n\u001b[0;32m    205\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torchvision\\datasets\\folder.py:104\u001b[0m, in \u001b[0;36mmake_dataset\u001b[1;34m(directory, class_to_idx, extensions, is_valid_file, allow_empty)\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m extensions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    103\u001b[0m         msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSupported extensions are: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mextensions\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28misinstance\u001b[39m(extensions,\u001b[38;5;250m \u001b[39m\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(extensions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 104\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(msg)\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m instances\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Found no valid file for the classes nobody, other_person, owner, pet. Supported extensions are: .jpg, .jpeg, .png, .ppm, .bmp, .pgm, .tif, .tiff, .webp"
     ]
    }
   ],
   "source": [
    "# Define transformations for preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to 224x224\n",
    "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n",
    "])\n",
    "\n",
    "# Load the dataset\n",
    "train_dataset = datasets.ImageFolder(root='../data', transform=transform)\n",
    "val_dataset = datasets.ImageFolder(root='../data', transform=transform)\n",
    "\n",
    "# Create DataLoader for training\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "\n",
    "# Create DataLoader for validation\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940fee7e-1180-4e67-b739-123dfb0de7f3",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c00727-fb47-4b0a-953c-66fcbc7d0e5a",
   "metadata": {},
   "source": [
    "## üß† Model Training Loop Explained\n",
    "\n",
    "This section explains the logic behind training our deep learning model using PyTorch.\n",
    "\n",
    "### üîÅ Epochs\n",
    "\n",
    "We train the model over **100 epochs** ‚Äî one epoch is a complete pass over the entire training dataset. Repeating this allows the model to learn gradually and improve its performance over time.\n",
    "\n",
    "### üìä Metrics Tracked\n",
    "\n",
    "During training, we monitor two key metrics every epoch:\n",
    "- **Loss** (üìâ): Measures how far off the model‚Äôs predictions are from the actual labels.\n",
    "- **Accuracy** (‚úÖ): Tells us how many predictions were correct out of the total.\n",
    "\n",
    "These are stored in `train_losses` and `train_accuracies` lists to help us visualize progress later.\n",
    "\n",
    "### üîÑ Training Steps Per Epoch\n",
    "\n",
    "Each epoch consists of several steps that happen for every batch of images:\n",
    "\n",
    "1. **Set model to training mode** üèãÔ∏è  \n",
    "   Enables dropout and batch normalization, which behave differently during training.\n",
    "\n",
    "2. **Loop through batches of data** üì¶  \n",
    "   We use `tqdm` to wrap our DataLoader, which gives us a nice real-time progress bar.\n",
    "\n",
    "3. **Zero out gradients** üßΩ  \n",
    "   We reset gradients using `optimizer.zero_grad()` so that past gradient values don‚Äôt accumulate.\n",
    "\n",
    "4. **Forward pass** üì§  \n",
    "   The input images are passed through the model to generate predictions.\n",
    "\n",
    "5. **Calculate loss** üìè  \n",
    "   We use a loss function (CrossEntropyLoss) to compute how wrong the model was.\n",
    "\n",
    "6. **Backward pass** üßÆ  \n",
    "   We call `.backward()` on the loss to compute gradients of the model parameters.\n",
    "\n",
    "7. **Update weights** üîß  \n",
    "   The optimizer (Adam) updates the model parameters based on the gradients with `optimizer.step()`.\n",
    "\n",
    "8. **Track loss and accuracy** üßæ  \n",
    "   We add the loss to a running total and count how many predictions were correct.\n",
    "\n",
    "### üßÆ After Each Epoch\n",
    "\n",
    "Once all batches are processed in an epoch:\n",
    "- We calculate the **average loss** and **accuracy** for that epoch.\n",
    "- We print this info to the console.\n",
    "- We append the values to our tracking lists for later use.\n",
    "\n",
    "### üíæ Saving the Model\n",
    "\n",
    "At the end of training, we save the learned weights of our model to a file `entity_classifier.pth`. This allows us to reuse the model later without retraining it from scratch! üí°\n",
    "\n",
    "### üìà Plotting Metrics\n",
    "\n",
    "We use `matplotlib` to generate two side-by-side plots:\n",
    "- **Training Loss per Epoch** (in red)\n",
    "- **Training Accuracy per Epoch** (in green)\n",
    "\n",
    "These plots visually show how well the model learned over time ‚Äî ideally, loss should decrease while accuracy increases! üìâüìà"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fa3425-a4cf-470d-b1f1-a5d39dc7453d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100  # We train the model for 100 epochs\n",
    "\n",
    "# We keep track of the training loss and accuracy for each epoch\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0  # Accumulates total loss in the epoch\n",
    "    correct_predictions = 0  # Tracks correct predictions\n",
    "    total_samples = 0  # Tracks number of samples seen\n",
    "\n",
    "    # tqdm progress bar for the current epoch\n",
    "    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)\n",
    "    for images, labels in loop:\n",
    "        optimizer.zero_grad()  # Clear gradients from previous step\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass to compute gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate the running loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Calculate number of correct predictions\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_samples += labels.size(0)\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "        # Update tqdm progress bar with current loss\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    # Calculate average loss and accuracy for the epoch\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    accuracy = 100 * correct_predictions / total_samples\n",
    "\n",
    "    # Store metrics for later visualization\n",
    "    train_losses.append(avg_loss)\n",
    "    train_accuracies.append(accuracy)\n",
    "\n",
    "    # Print epoch statistics\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# ‚úÖ Save the trained model for future use or inference\n",
    "torch.save(model.state_dict(), 'entity_classifier.pth')\n",
    "print(\"Model saved as 'entity_classifier.pth' ‚úÖ\")\n",
    "\n",
    "# üìà Plot training loss and accuracy over epochs using matplotlib\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot training loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label=\"Loss\", color=\"red\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# Plot training accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies, label=\"Accuracy\", color=\"green\")\n",
    "plt.title(\"Training Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# Show plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3594d8bd-8bbb-4bfe-abf6-20e7dc5eb221",
   "metadata": {},
   "source": [
    "üéâ **Success!** Our model is now fully trained and we have the metrics to prove it! üéâ\n",
    "\n",
    "We've completed the training process, and our model is ready for action. By evaluating the **training loss** and **accuracy** over the epochs, we can confirm the model‚Äôs progress and how well it has learned from the data.\n",
    "\n",
    "Now that we've achieved solid performance on the training set, it's time to move on to **real-time inference**! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18419def-a282-48a8-959b-cd918ec114a4",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f2048f-297f-46bf-8c78-a29c33339030",
   "metadata": {},
   "source": [
    "### What‚Äôs next? üëÄ\n",
    "Instead of continuing in a notebook, the next step is to integrate the trained model into a **PyCharm application** for real-time usage. We'll use the model to make predictions on live data, such as from a **webcam feed**. This allows us to test the model‚Äôs performance in real-world scenarios and see how it handles new, unseen data in an interactive application.\n",
    "\n",
    "Let‚Äôs take the model for a spin and see how it performs when it‚Äôs really put to work in a **PyCharm app**. üíª‚ú®"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
